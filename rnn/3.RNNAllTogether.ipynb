{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47a0a00e-4394-4c15-8f10-b9c54230b6db",
   "metadata": {},
   "source": [
    "# Full RNN Classifier in Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d680fd3e-e3a6-43f3-b06d-a25b7378b6f4",
   "metadata": {},
   "source": [
    "## a. Process Dataset\n",
    "1. Read Dataset\n",
    "2. Tokenize\n",
    "3. Build Vocab\n",
    "4. Numericalize\n",
    "5. Apply Transforms\n",
    "6. Batching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a53d0c-9bd1-451c-bc15-c8a7340f02db",
   "metadata": {},
   "source": [
    "## b. Define Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3250e5b0-a073-4ebf-b8a9-1757fd125e54",
   "metadata": {},
   "source": [
    "## c.Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cb62207-1b4e-4864-bebb-cb5f4b5a5c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.utils.data import DataLoader\n",
    "import torchtext.transforms as T\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2950ef5-e366-4c18-ab69-40c9b21d4c05",
   "metadata": {},
   "source": [
    "#### Build Iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea6a4fdc-bc70-492a-ae8c-e6109c22bf32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#If reading directly from a file\n",
    "# FILE_PATH = 'data/deu.txt'\n",
    "# data_pipe = dp.iter.IterableWrapper([FILE_PATH])\n",
    "# data_pipe = dp.iter.FileOpener(data_pipe, mode='rb')\n",
    "# data_pipe = data_pipe.parse_csv(skip_lines=0, delimiter='\\t', as_tuple=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6499f139-1135-42ed-a224-ba68a682d24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pipe=to_map_style_dataset(torchtext.datasets.AG_NEWS(split=('train')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39171a8d-1443-4854-986f-ebfc649da196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, \"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\")\n"
     ]
    }
   ],
   "source": [
    "for la in data_pipe:\n",
    "    print(la)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f2c5e4-fb53-4942-8650-f3b23fcac8c1",
   "metadata": {},
   "source": [
    "#### Tokenize - from a sentence to a list of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d84c616-7a37-47be-86b3-5494d0c61fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## lets Tokenize\n",
    "import spacy\n",
    "eng = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f00283f-ddcd-4c1b-9e0d-995d4669af76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi', 'how', 'are', 'you']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getTokens(text):\n",
    "    return [token.text for token in eng.tokenizer(text)]\n",
    "getTokens(\"Hi how are you\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fcb3d1-48eb-4bc2-ade1-b824cbb96773",
   "metadata": {},
   "source": [
    "#### Build Vocab\n",
    "build_vocab_from_iterator needs an iterator that yields a list of tokens, so we need to build that iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a30400d5-fc2b-4ee6-8745-c25fa0155a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokeniterator(data_pipe):\n",
    "    for label,text in data_pipe:\n",
    "        yield getTokens(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3659585f-46b9-461d-a0ca-c8dbc956bd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab=build_vocab_from_iterator(\n",
    "    tokeniterator(data_pipe),\n",
    "    min_freq=2,\n",
    "    specials= ['<pad>', '<sos>', '<eos>', '<unk>'],\n",
    "    special_first=True\n",
    ")\n",
    "vocab.set_default_index(vocab['<unk>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26c2dd74-c3d1-4da5-b710-3f3f944ffb74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[540, 27, 37, 6113]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab(['here', 'is', 'an', 'example'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba5b5ef2-2a42-41c5-8248-15e971afe3e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62544"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03200de8-c25a-487d-b7f3-92e0b114bd05",
   "metadata": {},
   "source": [
    "#### Numericalize -  using vocab\n",
    "So far our data_pipe is just an iterator that yields sentences, not even a list of tokens\n",
    "We will now transform the data_pipe to yield a list of indices of the tokesn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "78fc02d8-d306-4f79-97aa-de6e084e8afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getIndices(sample):\n",
    "    \"\"\"\n",
    "    Create transforms based on given vocabulary. The returned transform is applied to sequence\n",
    "    of tokens.\n",
    "    \"\"\"\n",
    "    text_tranform = T.Sequential(\n",
    "        ## converts the sentences to indices based on given vocabulary\n",
    "        T.VocabTransform(vocab=vocab),\n",
    "        ## Add <sos> at beginning of each sentence. 1 because the index for <sos> in vocabulary is\n",
    "        # 1 as seen in previous section\n",
    "        T.AddToken(1, begin=True),\n",
    "        ## Add <eos> at beginning of each sentence. 2 because the index for <eos> in vocabulary is\n",
    "        # 2 as seen in previous section\n",
    "        T.AddToken(2, begin=False)\n",
    "    )\n",
    "    label,text=sample\n",
    "    tokenized_text=getTokens(text)\n",
    "    transformed_indices=text_tranform(tokenized_text)\n",
    "    return transformed_indices,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f8cebdc-ec7b-4854-8b1f-afdceccca9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_pipe = data_pipe.map(getIndices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b65c968a-4479-4de4-8987-0cca9244cdf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for sample in data_pipe:\n",
    "#     print(sample)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86ae5aa-a063-4ade-831d-bc6db66f8e2c",
   "metadata": {},
   "source": [
    "## Important - Our sequences are of Variable Length and we are padding them using T.ToTensor(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d86c85e-5947-4a00-b250-8bae171ac692",
   "metadata": {},
   "source": [
    "#### Make Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "afea85b1-6820-46eb-bfcc-975c24e521ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    indices_and_labels=[getIndices(sample) for sample in batch]\n",
    "    tensors,targets=zip(*indices_and_labels)\n",
    "    ## `T.ToTensor(0)` returns a transform that converts the sequence to `torch.tensor` and also applies\n",
    "    # padding. Here, `0` is passed to the constructor to specify the index of the `<pad>` token in the\n",
    "    # vocabulary.\n",
    "    tensors=T.ToTensor(0)(list(tensors))\n",
    "    # ADJUST LABELS FROM 1,2,3,4 to 0,1,2,3\n",
    "    targets=[x-1 for x in targets]\n",
    "    targets=T.ToTensor(0)(list(targets))\n",
    "    return tensors,targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3793fcd0-8c24-4231-9ecf-580f414b974e",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64  # batch size for training\n",
    "train_iter=DataLoader(data_pipe,batch_size=BATCH_SIZE,collate_fn=collate_fn,shuffle=True,drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cb59e273-627d-421a-8309-dbfb1ca9f730",
   "metadata": {},
   "outputs": [],
   "source": [
    "i=iter(train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c5e8b44f-cafa-454e-a8be-4d364452eaca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[    1,    41, 10425,  ...,     0,     0,     0],\n",
       "         [    1,  1548,  2782,  ...,   278,    58,     2],\n",
       "         [    1,  3917,    78,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [    1,  2570,  3728,  ...,     0,     0,     0],\n",
       "         [    1,  1640,     7,  ...,     0,     0,     0],\n",
       "         [    1,  8962,   259,  ...,     0,     0,     0]]),\n",
       " tensor([0, 0, 1, 1, 3, 3, 2, 0, 0, 0, 3, 0, 2, 2, 1, 2, 0, 3, 3, 3, 1, 0, 2, 1,\n",
       "         2, 3, 0, 0, 2, 2, 1, 1, 0, 3, 0, 0, 3, 2, 1, 0, 0, 1, 1, 1, 1, 3, 2, 1,\n",
       "         2, 0, 1, 2, 1, 2, 3, 1, 3, 0, 2, 3, 0, 0, 2, 2]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c07f58-66c2-4c46-8a67-fe15c89e74cc",
   "metadata": {},
   "source": [
    "# Define Model - Figure out initializing hidden state and general RNN architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6364d30a-43a6-46fb-94ee-dc2755fa6d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7a1f2729-d56b-4568-a7e5-0af9159ab175",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProperRNN(nn.Module):\n",
    "    def __init__(self,vocab_size,embed_dim,hidden_size,num_classes):\n",
    "        super(ProperRNN,self).__init__()\n",
    "        self.hidden_size=hidden_size\n",
    "        self.embed=nn.Embedding(vocab_size,embed_dim)\n",
    "        self.rnn=nn.RNN(embed_dim,hidden_size,batch_first=True)\n",
    "        self.fc=nn.Linear(hidden_size,num_classes)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x=self.embed(x)\n",
    "        out,_=self.rnn(x,torch.randn(1*1,x.shape[0],self.hidden_size))\n",
    "        out=out[:,-1,:]\n",
    "        out=self.fc(out)\n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a3629f82-f5b9-411e-afa1-26c1a3230dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size=len(vocab)\n",
    "embed_dim=300\n",
    "hidden_size=10\n",
    "num_classes=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "568a50d2-d979-40ee-ba52-7c9eb2703e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prnn=ProperRNN(vocab_size,embed_dim,hidden_size,num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1d180366-2ef4-4275-ae8a-7d95b43299f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "o=prnn(next(i)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4fe565e7-9ffb-44a4-bbbb-5e8f18053184",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 4])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0094821-3dba-44f6-bf4a-8d9ab2567ca2",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d620739d-8686-47f5-8271-c82e771c9e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "EPOCHS = 10  # epoch\n",
    "LR = 5  # learning rate\n",
    "\n",
    "\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(prnn.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "371573e4-67b4-4042-bc7d-71cb1a573612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   500/ 1875 batches | accuracy    0.249\n",
      "| epoch   1 |  1000/ 1875 batches | accuracy    0.252\n",
      "| epoch   1 |  1500/ 1875 batches | accuracy    0.252\n",
      "| epoch   2 |   500/ 1875 batches | accuracy    0.251\n",
      "| epoch   2 |  1000/ 1875 batches | accuracy    0.248\n",
      "| epoch   2 |  1500/ 1875 batches | accuracy    0.251\n",
      "| epoch   3 |   500/ 1875 batches | accuracy    0.248\n",
      "| epoch   3 |  1000/ 1875 batches | accuracy    0.247\n",
      "| epoch   3 |  1500/ 1875 batches | accuracy    0.249\n",
      "| epoch   4 |   500/ 1875 batches | accuracy    0.250\n",
      "| epoch   4 |  1000/ 1875 batches | accuracy    0.252\n",
      "| epoch   4 |  1500/ 1875 batches | accuracy    0.247\n",
      "| epoch   5 |   500/ 1875 batches | accuracy    0.252\n",
      "| epoch   5 |  1000/ 1875 batches | accuracy    0.251\n",
      "| epoch   5 |  1500/ 1875 batches | accuracy    0.250\n"
     ]
    }
   ],
   "source": [
    "prnn.train()\n",
    "epochs=5\n",
    "correct_preds, total_count = 0, 0\n",
    "log_interval = 500\n",
    "start_time = time.time()\n",
    "for epoch in range(1,epochs+1):\n",
    "    for idx,(tensors,targets) in enumerate(train_iter):\n",
    "        optimizer.zero_grad()\n",
    "        predicted_labels=prnn(tensors)\n",
    "        loss=criterion(predicted_labels,targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        correct_preds +=(predicted_labels.argmax(1)==targets).sum().item()\n",
    "        total_count += targets.size(0)\n",
    "        if idx%log_interval==0 and idx>0:\n",
    "            elapsed=time.time() - start_time\n",
    "            print(\n",
    "            \"| epoch {:3d} | {:5d}/{:5d} batches \"\n",
    "                    \"| accuracy {:8.3f}\".format(\n",
    "                        ##### Weird - in order for len(train_iter) to work data_pipe must be converted using to_map_style_dataset\n",
    "                        #### after which you cannot do data_pipe.map(func) even more weird\n",
    "                        #### to circumvent the transform will need to go into the collate_fn\n",
    "                        #### so all transform inside collate_fn and then len(dataloader) will also work as long as data_pipe was converted\n",
    "                        #### to dataset using to_map_style_dataset\n",
    "                        epoch, idx, len(train_iter), correct_preds / total_count\n",
    "                    )\n",
    "                )\n",
    "            correct_preds, total_count = 0, 0\n",
    "            start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a132a9e3-83ce-43bc-ba4b-be9d450117f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
