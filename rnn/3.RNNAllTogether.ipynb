{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47a0a00e-4394-4c15-8f10-b9c54230b6db",
   "metadata": {},
   "source": [
    "# Full RNN Classifier in Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d680fd3e-e3a6-43f3-b06d-a25b7378b6f4",
   "metadata": {},
   "source": [
    "## a. Process Dataset\n",
    "1. Read Dataset\n",
    "2. Tokenize\n",
    "3. Build Vocab\n",
    "4. Numericalize\n",
    "5. Apply Transforms\n",
    "6. Batching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a53d0c-9bd1-451c-bc15-c8a7340f02db",
   "metadata": {},
   "source": [
    "## b. Define Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3250e5b0-a073-4ebf-b8a9-1757fd125e54",
   "metadata": {},
   "source": [
    "## c.Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cb62207-1b4e-4864-bebb-cb5f4b5a5c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.utils.data import DataLoader\n",
    "import torchtext.transforms as T\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61d86338-5bc3-49d2-b39e-39722c0f80b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not torch.backends.mps.is_available():\n",
    "    if not torch.backends.mps.is_built():\n",
    "        print(\"MPS not available because the current PyTorch install was not \"\n",
    "              \"built with MPS enabled.\")\n",
    "    else:\n",
    "        print(\"MPS not available because the current MacOS version is not 12.3+ \"\n",
    "              \"and/or you do not have an MPS-enabled device on this machine.\")\n",
    "else:\n",
    "    mps_device = torch.device(\"mps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2950ef5-e366-4c18-ab69-40c9b21d4c05",
   "metadata": {},
   "source": [
    "#### Build Iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea6a4fdc-bc70-492a-ae8c-e6109c22bf32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#If reading directly from a file\n",
    "# FILE_PATH = 'data/deu.txt'\n",
    "# data_pipe = dp.iter.IterableWrapper([FILE_PATH])\n",
    "# data_pipe = dp.iter.FileOpener(data_pipe, mode='rb')\n",
    "# data_pipe = data_pipe.parse_csv(skip_lines=0, delimiter='\\t', as_tuple=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6499f139-1135-42ed-a224-ba68a682d24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pipe=to_map_style_dataset(torchtext.datasets.AG_NEWS(split=('train')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39171a8d-1443-4854-986f-ebfc649da196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, \"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\")\n"
     ]
    }
   ],
   "source": [
    "for la in data_pipe:\n",
    "    print(la)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f2c5e4-fb53-4942-8650-f3b23fcac8c1",
   "metadata": {},
   "source": [
    "#### Tokenize - from a sentence to a list of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d84c616-7a37-47be-86b3-5494d0c61fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## lets Tokenize\n",
    "import spacy\n",
    "eng = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f00283f-ddcd-4c1b-9e0d-995d4669af76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi', 'how', 'are', 'you']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getTokens(text):\n",
    "    return [token.text for token in eng.tokenizer(text)]\n",
    "getTokens(\"Hi how are you\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fcb3d1-48eb-4bc2-ade1-b824cbb96773",
   "metadata": {},
   "source": [
    "#### Build Vocab\n",
    "build_vocab_from_iterator needs an iterator that yields a list of tokens, so we need to build that iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a30400d5-fc2b-4ee6-8745-c25fa0155a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokeniterator(data_pipe):\n",
    "    for label,text in data_pipe:\n",
    "        yield getTokens(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3659585f-46b9-461d-a0ca-c8dbc956bd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab=build_vocab_from_iterator(\n",
    "    tokeniterator(data_pipe),\n",
    "    min_freq=2,\n",
    "    specials= ['<pad>', '<sos>', '<eos>', '<unk>'],\n",
    "    special_first=True\n",
    ")\n",
    "vocab.set_default_index(vocab['<unk>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26c2dd74-c3d1-4da5-b710-3f3f944ffb74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[540, 27, 37, 6113]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab(['here', 'is', 'an', 'example'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba5b5ef2-2a42-41c5-8248-15e971afe3e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62544"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03200de8-c25a-487d-b7f3-92e0b114bd05",
   "metadata": {},
   "source": [
    "#### Numericalize -  using vocab\n",
    "So far our data_pipe is just an iterator that yields sentences, not even a list of tokens\n",
    "We will now transform the data_pipe to yield a list of indices of the tokesn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "78fc02d8-d306-4f79-97aa-de6e084e8afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getIndices(sample):\n",
    "    \"\"\"\n",
    "    Create transforms based on given vocabulary. The returned transform is applied to sequence\n",
    "    of tokens.\n",
    "    \"\"\"\n",
    "    text_tranform = T.Sequential(\n",
    "        ## converts the sentences to indices based on given vocabulary\n",
    "        T.VocabTransform(vocab=vocab),\n",
    "        ## Add <sos> at beginning of each sentence. 1 because the index for <sos> in vocabulary is\n",
    "        # 1 as seen in previous section\n",
    "        T.AddToken(1, begin=True),\n",
    "        ## Add <eos> at beginning of each sentence. 2 because the index for <eos> in vocabulary is\n",
    "        # 2 as seen in previous section\n",
    "        T.AddToken(2, begin=False)\n",
    "    )\n",
    "    label,text=sample\n",
    "    tokenized_text=getTokens(text)\n",
    "    transformed_indices=text_tranform(tokenized_text)\n",
    "    return transformed_indices,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f8cebdc-ec7b-4854-8b1f-afdceccca9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_pipe = data_pipe.map(getIndices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b65c968a-4479-4de4-8987-0cca9244cdf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for sample in data_pipe:\n",
    "#     print(sample)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86ae5aa-a063-4ade-831d-bc6db66f8e2c",
   "metadata": {},
   "source": [
    "## Important - Our sequences are of Variable Length and we are padding them using T.ToTensor(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d86c85e-5947-4a00-b250-8bae171ac692",
   "metadata": {},
   "source": [
    "#### Make Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "afea85b1-6820-46eb-bfcc-975c24e521ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    indices_and_labels=[getIndices(sample) for sample in batch]\n",
    "    tensors,targets=zip(*indices_and_labels)\n",
    "    ## `T.ToTensor(0)` returns a transform that converts the sequence to `torch.tensor` and also applies\n",
    "    # padding. Here, `0` is passed to the constructor to specify the index of the `<pad>` token in the\n",
    "    # vocabulary.\n",
    "    tensor_lengths=torch.tensor([len(t) for t in tensors])\n",
    "    tensors=T.ToTensor(0)(list(tensors))\n",
    "    # ADJUST LABELS FROM 1,2,3,4 to 0,1,2,3\n",
    "    targets=[x-1 for x in targets]\n",
    "    targets=T.ToTensor(0)(list(targets))\n",
    "    return tensors,tensor_lengths,targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "91d0b46a-8b04-4eee-9f87-83c1f9472add",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[    1,  7963,  3194,  1682, 17642,     2],\n",
       "         [    1,  7963,  3194,   256,     2,     0]]),\n",
       " tensor([6, 5]),\n",
       " tensor([0, 1]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs,lengths,targets=collate_fn(\n",
    "    [\n",
    "        [1,\"i am sports op\"],\n",
    "        [2,\"i am news\"]\n",
    "    ]\n",
    ")\n",
    "inputs,lengths,targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fd6363c2-71e0-45ee-b413-573d1174966c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size=10\n",
    "rn=nn.LSTM(1,hidden_size,batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9561a3b3-8de0-4587-97d6-8ba2c550f0e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gl/sw0j9jdj3z7_8nx2zjs29z_5yg7h25/T/ipykernel_96794/911366731.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  inputs=torch.tensor(inputs,dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "inputs=torch.tensor(inputs,dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "988328ec-7e29-4ab4-a056-0f45e2476ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs=inputs.reshape(inputs.shape[0],inputs.shape[1],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8a235b7c-e84d-4c01-ba4e-47a3ec892aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "pps=torch.nn.utils.rnn.pack_padded_sequence(inputs,lengths,batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2d639091-62dc-4a46-9986-c6467d781f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pps.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0ab8cf26-2956-4d30-ac27-63a369dd490a",
   "metadata": {},
   "outputs": [],
   "source": [
    "oo=rn(pps,(torch.randn(1*1,inputs.shape[0],hidden_size),torch.randn(1*1,inputs.shape[0],hidden_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cba63eaf-936b-40f7-bc27-a6578225bf16",
   "metadata": {},
   "outputs": [],
   "source": [
    "ooo,(hhh,_)=oo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3d08a5b0-6a19-4729-aabe-9d85cff829d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hhh.reshape(2,10).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0a13fff1-f775-4131-b429-a9a12aa39473",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.utils.rnn.pad_packed_sequence(ooo,batch_first=True)[0][:,-1,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "88388d9c-5870-4260-b890-239796ca10d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fcc=nn.Linear(10,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d8a57e31-9f24-4fc6-b21b-3f5ae546a626",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[-0.0827, -0.1664,  0.0024, -0.3924,  0.1788,  0.1572,  0.2666, -0.0468,\n",
       "          -0.2979,  0.1421],\n",
       "         [-0.2055,  0.0000, -0.0000, -0.8609,  0.0000, -0.5236,  0.0000,  0.0000,\n",
       "          -0.0000,  0.3793],\n",
       "         [-0.2055,  0.0000, -0.0000, -0.8609,  0.0000, -0.9188,  0.0000,  0.0000,\n",
       "          -0.0000,  0.3793],\n",
       "         [-0.2055,  0.0000, -0.0000, -0.8609,  0.0000, -0.9886,  0.0000,  0.0000,\n",
       "          -0.0000,  0.3793],\n",
       "         [-0.2055,  0.0000, -0.0000, -0.8609,  0.0000, -0.9985,  0.0000,  0.0000,\n",
       "          -0.0000,  0.3793],\n",
       "         [ 0.0139,  0.3212, -0.5118, -0.4108,  0.2276, -0.4654,  0.1440,  0.2449,\n",
       "          -0.0964,  0.2409]], grad_fn=<IndexBackward0>),\n",
       " tensor([[ 2.6629e-01,  9.1640e-03, -9.3617e-02,  1.5350e-01, -9.0192e-02,\n",
       "           7.4383e-02,  2.2065e-01, -1.6623e-02,  1.1016e-01,  2.3466e-01],\n",
       "         [ 4.4213e-01,  0.0000e+00, -0.0000e+00,  2.5275e-01,  0.0000e+00,\n",
       "          -6.6914e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00,  3.9761e-01],\n",
       "         [ 4.4213e-01,  0.0000e+00, -0.0000e+00,  2.5275e-01,  0.0000e+00,\n",
       "          -9.4775e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00,  3.9761e-01],\n",
       "         [ 4.4213e-01,  2.5496e-11, -2.0911e-08,  2.5204e-01,  4.4147e-33,\n",
       "          -9.9275e-01,  0.0000e+00,  3.7903e-12, -1.4957e-20,  3.9761e-01],\n",
       "         [ 2.6835e-01,  3.2596e-01, -5.4042e-01, -2.4029e-02,  1.6739e-01,\n",
       "          -4.5928e-01,  1.1722e-01,  2.4081e-01, -6.8511e-02,  2.6955e-01]],\n",
       "        grad_fn=<IndexBackward0>)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.utils.rnn.unpack_sequence(ooo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5316aa16-8d61-46a8-8ffb-b3ef41321d30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2745, -0.4486, -0.1028,  0.0007],\n",
       "        [ 0.5427, -0.5159, -0.2167,  0.0884],\n",
       "        [ 0.6368, -0.5869, -0.3198, -0.0343],\n",
       "        [ 0.6534, -0.5995, -0.3380, -0.0560],\n",
       "        [ 0.6558, -0.6012, -0.3406, -0.0590],\n",
       "        [ 0.6005, -0.3504, -0.2885, -0.0825]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fcc(nn.utils.rnn.unpack_sequence(ooo)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "15671f00-0552-4c68-a20b-bd65f30d2988",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[-0.0827, -0.1664,  0.0024, -0.3924,  0.1788,  0.1572,  0.2666, -0.0468,\n",
       "          -0.2979,  0.1421],\n",
       "         [-0.2055,  0.0000, -0.0000, -0.8609,  0.0000, -0.5236,  0.0000,  0.0000,\n",
       "          -0.0000,  0.3793],\n",
       "         [-0.2055,  0.0000, -0.0000, -0.8609,  0.0000, -0.9188,  0.0000,  0.0000,\n",
       "          -0.0000,  0.3793],\n",
       "         [-0.2055,  0.0000, -0.0000, -0.8609,  0.0000, -0.9886,  0.0000,  0.0000,\n",
       "          -0.0000,  0.3793],\n",
       "         [-0.2055,  0.0000, -0.0000, -0.8609,  0.0000, -0.9985,  0.0000,  0.0000,\n",
       "          -0.0000,  0.3793],\n",
       "         [ 0.0139,  0.3212, -0.5118, -0.4108,  0.2276, -0.4654,  0.1440,  0.2449,\n",
       "          -0.0964,  0.2409]], grad_fn=<IndexBackward0>),\n",
       " tensor([[ 2.6629e-01,  9.1640e-03, -9.3617e-02,  1.5350e-01, -9.0192e-02,\n",
       "           7.4383e-02,  2.2065e-01, -1.6623e-02,  1.1016e-01,  2.3466e-01],\n",
       "         [ 4.4213e-01,  0.0000e+00, -0.0000e+00,  2.5275e-01,  0.0000e+00,\n",
       "          -6.6914e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00,  3.9761e-01],\n",
       "         [ 4.4213e-01,  0.0000e+00, -0.0000e+00,  2.5275e-01,  0.0000e+00,\n",
       "          -9.4775e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00,  3.9761e-01],\n",
       "         [ 4.4213e-01,  2.5496e-11, -2.0911e-08,  2.5204e-01,  4.4147e-33,\n",
       "          -9.9275e-01,  0.0000e+00,  3.7903e-12, -1.4957e-20,  3.9761e-01],\n",
       "         [ 2.6835e-01,  3.2596e-01, -5.4042e-01, -2.4029e-02,  1.6739e-01,\n",
       "          -4.5928e-01,  1.1722e-01,  2.4081e-01, -6.8511e-02,  2.6955e-01]],\n",
       "        grad_fn=<IndexBackward0>)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.utils.rnn.unpack_sequence(ooo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c07f58-66c2-4c46-8a67-fe15c89e74cc",
   "metadata": {},
   "source": [
    "# Define Model - Figure out initializing hidden state and general RNN architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0094821-3dba-44f6-bf4a-8d9ab2567ca2",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8562cb6a-8d10-4868-a857-a7c32078137d",
   "metadata": {},
   "source": [
    "# Important Observations - RNN Model\n",
    "## 1. Train Accuracy jumped from unmoving 25% to about 55% after feeding a packed sequence to the RNN.\n",
    "## It took some work to make use of the packed sequence that is the output of the RNN. So decided to \n",
    "## use the hidden layer instead after reshaping, then feed to the linear layer. The accuracy jump\n",
    "## shows promise and does make sense, we are now skipping the padded indices during the forward and backward pass\n",
    "## which were probably adding a lot of noise causing the accuracy to be at 25%\n",
    "\n",
    "## Next step will be to try LSTM and see if the long and short memory helps achieve better accuracy scores\n",
    "\n",
    "# 2. LSTM has WON! 96.2 % Accuracy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2a0cbc44-2cf6-4d33-8d4c-deff0f707b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64  # batch size for training\n",
    "train_iter=DataLoader(data_pipe,batch_size=BATCH_SIZE,collate_fn=collate_fn,shuffle=True,drop_last=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7a1f2729-d56b-4568-a7e5-0af9159ab175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   500/ 1875 batches | accuracy    0.253\n",
      "| epoch   1 |  1000/ 1875 batches | accuracy    0.266\n",
      "| epoch   1 |  1500/ 1875 batches | accuracy    0.323\n",
      "| epoch   2 |   500/ 1875 batches | accuracy    0.389\n",
      "| epoch   2 |  1000/ 1875 batches | accuracy    0.431\n",
      "| epoch   2 |  1500/ 1875 batches | accuracy    0.464\n",
      "| epoch   3 |   500/ 1875 batches | accuracy    0.485\n",
      "| epoch   3 |  1000/ 1875 batches | accuracy    0.495\n",
      "| epoch   3 |  1500/ 1875 batches | accuracy    0.503\n",
      "| epoch   4 |   500/ 1875 batches | accuracy    0.508\n",
      "| epoch   4 |  1000/ 1875 batches | accuracy    0.517\n",
      "| epoch   4 |  1500/ 1875 batches | accuracy    0.524\n",
      "| epoch   5 |   500/ 1875 batches | accuracy    0.525\n",
      "| epoch   5 |  1000/ 1875 batches | accuracy    0.533\n",
      "| epoch   5 |  1500/ 1875 batches | accuracy    0.533\n",
      "| epoch   6 |   500/ 1875 batches | accuracy    0.542\n",
      "| epoch   6 |  1000/ 1875 batches | accuracy    0.542\n",
      "| epoch   6 |  1500/ 1875 batches | accuracy    0.549\n",
      "| epoch   7 |   500/ 1875 batches | accuracy    0.540\n",
      "| epoch   7 |  1000/ 1875 batches | accuracy    0.553\n",
      "| epoch   7 |  1500/ 1875 batches | accuracy    0.547\n",
      "| epoch   8 |   500/ 1875 batches | accuracy    0.543\n",
      "| epoch   8 |  1000/ 1875 batches | accuracy    0.551\n",
      "| epoch   8 |  1500/ 1875 batches | accuracy    0.546\n",
      "| epoch   9 |   500/ 1875 batches | accuracy    0.556\n",
      "| epoch   9 |  1000/ 1875 batches | accuracy    0.549\n",
      "| epoch   9 |  1500/ 1875 batches | accuracy    0.552\n",
      "| epoch  10 |   500/ 1875 batches | accuracy    0.548\n",
      "| epoch  10 |  1000/ 1875 batches | accuracy    0.539\n",
      "| epoch  10 |  1500/ 1875 batches | accuracy    0.549\n"
     ]
    }
   ],
   "source": [
    "class ProperRNN(nn.Module):\n",
    "    def __init__(self,vocab_size,embed_dim,hidden_size,num_classes):\n",
    "        super(ProperRNN,self).__init__()\n",
    "        self.hidden_size=hidden_size\n",
    "        self.embedding=nn.Embedding(vocab_size,embed_dim)\n",
    "        self.rnn=nn.RNN(embed_dim,hidden_size,batch_first=True)\n",
    "        self.fc=nn.Linear(hidden_size,num_classes)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.rnn.weight_ih_l0.data.uniform_(-initrange, initrange)\n",
    "        self.rnn.bias_ih_l0.data.zero_()\n",
    "        self.fc.bias.data.zero_()\n",
    "    \n",
    "    def forward(self,text_tensors,text_lengths):\n",
    "        text_tensors=self.embedding(text_tensors)\n",
    "        pps=torch.nn.utils.rnn.pack_padded_sequence(text_tensors,text_lengths,batch_first=True,enforce_sorted=False)\n",
    "        out,hhh=self.rnn(pps,torch.randn(1*1,text_tensors.shape[0],self.hidden_size))\n",
    "        # out\n",
    "        # out=out[:,-1,:]\n",
    "        hhh=hhh.reshape(len(text_lengths),self.hidden_size)\n",
    "        out=self.fc(hhh)\n",
    "        return out\n",
    "\n",
    "vocab_size=len(vocab)\n",
    "embed_dim=300\n",
    "hidden_size=10\n",
    "num_classes=4\n",
    "prnn=ProperRNN(vocab_size,embed_dim,hidden_size,num_classes)\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "EPOCHS = 10  # epoch\n",
    "LR = 5  # learning rate\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(prnn.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
    "\n",
    "##### TRAIN #####\n",
    "prnn.train()\n",
    "correct_preds, total_count = 0, 0\n",
    "log_interval = 500\n",
    "start_time = time.time()\n",
    "for epoch in range(1,EPOCHS+1):\n",
    "    for idx,(tensors,tensor_lengths,targets) in enumerate(train_iter):\n",
    "        optimizer.zero_grad()\n",
    "        predicted_labels=prnn(tensors,tensor_lengths)\n",
    "        loss=criterion(predicted_labels,targets)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(prnn.parameters(), 0.1)\n",
    "        optimizer.step()\n",
    "        correct_preds +=(predicted_labels.argmax(1)==targets).sum().item()\n",
    "        total_count += targets.size(0)\n",
    "        if idx%log_interval==0 and idx>0:\n",
    "            elapsed=time.time() - start_time\n",
    "            print(\n",
    "            \"| epoch {:3d} | {:5d}/{:5d} batches \"\n",
    "                    \"| accuracy {:8.3f}\".format(\n",
    "                        ##### Weird - in order for len(train_iter) to work data_pipe must be converted using to_map_style_dataset\n",
    "                        #### after which you cannot do data_pipe.map(func) even more weird\n",
    "                        #### to circumvent the transform will need to go into the collate_fn\n",
    "                        #### so all transform inside collate_fn and then len(dataloader) will also work as long as data_pipe was converted\n",
    "                        #### to dataset using to_map_style_dataset\n",
    "                        epoch, idx, len(train_iter), correct_preds / total_count\n",
    "                    )\n",
    "                )\n",
    "            correct_preds, total_count = 0, 0\n",
    "            start_time = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "82f92e98-c117-45ba-b0c9-50d32b5d3abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   500/ 1875 batches | accuracy    0.493\n",
      "| epoch   1 |  1000/ 1875 batches | accuracy    0.787\n",
      "| epoch   1 |  1500/ 1875 batches | accuracy    0.846\n",
      "| epoch   2 |   500/ 1875 batches | accuracy    0.870\n",
      "| epoch   2 |  1000/ 1875 batches | accuracy    0.881\n",
      "| epoch   2 |  1500/ 1875 batches | accuracy    0.886\n",
      "| epoch   3 |   500/ 1875 batches | accuracy    0.895\n",
      "| epoch   3 |  1000/ 1875 batches | accuracy    0.906\n",
      "| epoch   3 |  1500/ 1875 batches | accuracy    0.901\n",
      "| epoch   4 |   500/ 1875 batches | accuracy    0.914\n",
      "| epoch   4 |  1000/ 1875 batches | accuracy    0.917\n",
      "| epoch   4 |  1500/ 1875 batches | accuracy    0.916\n",
      "| epoch   5 |   500/ 1875 batches | accuracy    0.926\n",
      "| epoch   5 |  1000/ 1875 batches | accuracy    0.928\n",
      "| epoch   5 |  1500/ 1875 batches | accuracy    0.927\n",
      "| epoch   6 |   500/ 1875 batches | accuracy    0.935\n",
      "| epoch   6 |  1000/ 1875 batches | accuracy    0.936\n",
      "| epoch   6 |  1500/ 1875 batches | accuracy    0.936\n",
      "| epoch   7 |   500/ 1875 batches | accuracy    0.940\n",
      "| epoch   7 |  1000/ 1875 batches | accuracy    0.947\n",
      "| epoch   7 |  1500/ 1875 batches | accuracy    0.943\n",
      "| epoch   8 |   500/ 1875 batches | accuracy    0.950\n",
      "| epoch   8 |  1000/ 1875 batches | accuracy    0.951\n",
      "| epoch   8 |  1500/ 1875 batches | accuracy    0.950\n",
      "| epoch   9 |   500/ 1875 batches | accuracy    0.957\n",
      "| epoch   9 |  1000/ 1875 batches | accuracy    0.960\n",
      "| epoch   9 |  1500/ 1875 batches | accuracy    0.956\n",
      "| epoch  10 |   500/ 1875 batches | accuracy    0.960\n",
      "| epoch  10 |  1000/ 1875 batches | accuracy    0.965\n",
      "| epoch  10 |  1500/ 1875 batches | accuracy    0.962\n"
     ]
    }
   ],
   "source": [
    "class ProperLSTM(nn.Module):\n",
    "    def __init__(self,vocab_size,embed_dim,hidden_size,num_classes):\n",
    "        super(ProperLSTM,self).__init__()\n",
    "        self.hidden_size=hidden_size\n",
    "        self.embedding=nn.Embedding(vocab_size,embed_dim)\n",
    "        self.rnn=nn.LSTM(embed_dim,hidden_size,batch_first=True)\n",
    "        self.fc=nn.Linear(hidden_size,num_classes)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.rnn.weight_ih_l0.data.uniform_(-initrange, initrange)\n",
    "        self.rnn.bias_ih_l0.data.zero_()\n",
    "        self.fc.bias.data.zero_()\n",
    "    \n",
    "    def forward(self,text_tensors,text_lengths):\n",
    "        text_tensors=self.embedding(text_tensors)\n",
    "        pps=torch.nn.utils.rnn.pack_padded_sequence(text_tensors,text_lengths,batch_first=True,enforce_sorted=False)\n",
    "        out,(hhh,_)=self.rnn(pps,(torch.randn(1*1,text_tensors.shape[0],self.hidden_size),torch.randn(1*1,text_tensors.shape[0],self.hidden_size)))\n",
    "        # out\n",
    "        # out=out[:,-1,:]\n",
    "        hhh=hhh.reshape(len(text_lengths),self.hidden_size)\n",
    "        out=self.fc(hhh)\n",
    "        return out\n",
    "\n",
    "vocab_size=len(vocab)\n",
    "embed_dim=300\n",
    "hidden_size=10\n",
    "num_classes=4\n",
    "plstm=ProperLSTM(vocab_size,embed_dim,hidden_size,num_classes)\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "EPOCHS = 10  # epoch\n",
    "LR = 5  # learning rate\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(plstm.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
    "\n",
    "##### TRAIN #####\n",
    "plstm.train()\n",
    "correct_preds, total_count = 0, 0\n",
    "log_interval = 500\n",
    "start_time = time.time()\n",
    "for epoch in range(1,EPOCHS+1):\n",
    "    for idx,(tensors,tensor_lengths,targets) in enumerate(train_iter):\n",
    "        optimizer.zero_grad()\n",
    "        predicted_labels=plstm(tensors,tensor_lengths)\n",
    "        loss=criterion(predicted_labels,targets)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(plstm.parameters(), 0.1)\n",
    "        optimizer.step()\n",
    "        correct_preds +=(predicted_labels.argmax(1)==targets).sum().item()\n",
    "        total_count += targets.size(0)\n",
    "        if idx%log_interval==0 and idx>0:\n",
    "            elapsed=time.time() - start_time\n",
    "            print(\n",
    "            \"| epoch {:3d} | {:5d}/{:5d} batches \"\n",
    "                    \"| accuracy {:8.3f}\".format(\n",
    "                        ##### Weird - in order for len(train_iter) to work data_pipe must be converted using to_map_style_dataset\n",
    "                        #### after which you cannot do data_pipe.map(func) even more weird\n",
    "                        #### to circumvent the transform will need to go into the collate_fn\n",
    "                        #### so all transform inside collate_fn and then len(dataloader) will also work as long as data_pipe was converted\n",
    "                        #### to dataset using to_map_style_dataset\n",
    "                        epoch, idx, len(train_iter), correct_preds / total_count\n",
    "                    )\n",
    "                )\n",
    "            correct_preds, total_count = 0, 0\n",
    "            start_time = time.time()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c8c249",
   "metadata": {},
   "source": [
    "# Important Observations - Embedding Bag Model\n",
    "## 1. Train Accuracy jumped from unmoving 25% to about 78% after 10 epochs when applied the learning rate changes\n",
    "```\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
    "```\n",
    "insert clip gradient norm between backward and step\n",
    "```\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "        optimizer.step()\n",
    "```\n",
    "## 2. Train Accuracy jumped from 78% to 96% on passing in the padding_idx to Embedding Bag.\n",
    "Obviously when the embedding bag sums/means the weights of the embeddings\n",
    "we want it to ignore the weights of the padding index\n",
    "padding is just a convenience feature for pytorch training\n",
    "```\n",
    "self.embedding=nn.EmbeddingBag(vocab_size,embed_dim,padding_idx=0)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a132a9e3-83ce-43bc-ba4b-be9d450117f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   500/ 1875 batches | accuracy    0.715\n",
      "| epoch   1 |  1000/ 1875 batches | accuracy    0.854\n",
      "| epoch   1 |  1500/ 1875 batches | accuracy    0.877\n",
      "| epoch   2 |   500/ 1875 batches | accuracy    0.895\n",
      "| epoch   2 |  1000/ 1875 batches | accuracy    0.902\n",
      "| epoch   2 |  1500/ 1875 batches | accuracy    0.904\n",
      "| epoch   3 |   500/ 1875 batches | accuracy    0.915\n",
      "| epoch   3 |  1000/ 1875 batches | accuracy    0.920\n",
      "| epoch   3 |  1500/ 1875 batches | accuracy    0.919\n",
      "| epoch   4 |   500/ 1875 batches | accuracy    0.926\n",
      "| epoch   4 |  1000/ 1875 batches | accuracy    0.929\n",
      "| epoch   4 |  1500/ 1875 batches | accuracy    0.929\n",
      "| epoch   5 |   500/ 1875 batches | accuracy    0.936\n",
      "| epoch   5 |  1000/ 1875 batches | accuracy    0.935\n",
      "| epoch   5 |  1500/ 1875 batches | accuracy    0.935\n",
      "| epoch   6 |   500/ 1875 batches | accuracy    0.939\n",
      "| epoch   6 |  1000/ 1875 batches | accuracy    0.942\n",
      "| epoch   6 |  1500/ 1875 batches | accuracy    0.941\n",
      "| epoch   7 |   500/ 1875 batches | accuracy    0.944\n",
      "| epoch   7 |  1000/ 1875 batches | accuracy    0.946\n",
      "| epoch   7 |  1500/ 1875 batches | accuracy    0.943\n",
      "| epoch   8 |   500/ 1875 batches | accuracy    0.949\n",
      "| epoch   8 |  1000/ 1875 batches | accuracy    0.951\n",
      "| epoch   8 |  1500/ 1875 batches | accuracy    0.950\n",
      "| epoch   9 |   500/ 1875 batches | accuracy    0.951\n",
      "| epoch   9 |  1000/ 1875 batches | accuracy    0.954\n",
      "| epoch   9 |  1500/ 1875 batches | accuracy    0.951\n",
      "| epoch  10 |   500/ 1875 batches | accuracy    0.954\n",
      "| epoch  10 |  1000/ 1875 batches | accuracy    0.956\n",
      "| epoch  10 |  1500/ 1875 batches | accuracy    0.956\n"
     ]
    }
   ],
   "source": [
    "class EmbeddingBagClassifier(nn.Module):\n",
    "    def __init__(self,vocab_size,embed_dim,num_classes):\n",
    "        super(EmbeddingBagClassifier,self).__init__()\n",
    "        self.embedding=nn.EmbeddingBag(vocab_size,embed_dim,padding_idx=0)\n",
    "        self.fc=nn.Linear(embed_dim,num_classes)\n",
    "        self.init_weights()\n",
    "\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "    def forward(self,x):\n",
    "        x=self.embedding(x)\n",
    "        out=self.fc(x)\n",
    "        return out\n",
    "\n",
    "vocab_size=len(vocab)\n",
    "embed_dim=300\n",
    "num_classes=4\n",
    "embedbagmodel=EmbeddingBagClassifier(vocab_size,embed_dim,num_classes)\n",
    "\n",
    "# Hyperparameters\n",
    "EPOCHS = 10  # epoch\n",
    "LR = 5  # learning rate\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(embedbagmodel.parameters(), lr=LR, momentum=0.1)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
    "\n",
    "#### TRAIN#####\n",
    "embedbagmodel.train()\n",
    "epochs=10\n",
    "correct_preds, total_count = 0, 0\n",
    "log_interval = 500\n",
    "start_time = time.time()\n",
    "for epoch in range(1,epochs+1):\n",
    "    for idx,(tensors,tensor_lengths,targets) in enumerate(train_iter):\n",
    "        tensors,targets=tensors,targets\n",
    "        optimizer.zero_grad()\n",
    "        predicted_labels=embedbagmodel(tensors)\n",
    "        loss=criterion(predicted_labels,targets)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(embedbagmodel.parameters(), 0.1)\n",
    "        optimizer.step()\n",
    "        correct_preds +=(predicted_labels.argmax(1)==targets).sum().item()\n",
    "        total_count += targets.size(0)\n",
    "        if idx%log_interval==0 and idx>0:\n",
    "            elapsed=time.time() - start_time\n",
    "            print(\n",
    "            \"| epoch {:3d} | {:5d}/{:5d} batches \"\n",
    "                    \"| accuracy {:8.3f}\".format(\n",
    "                        ##### Weird - in order for len(train_iter) to work data_pipe must be converted using to_map_style_dataset\n",
    "                        #### after which you cannot do data_pipe.map(func) even more weird\n",
    "                        #### to circumvent the transform will need to go into the collate_fn\n",
    "                        #### so all transform inside collate_fn and then len(dataloader) will also work as long as data_pipe was converted\n",
    "                        #### to dataset using to_map_style_dataset\n",
    "                        epoch, idx, len(train_iter), correct_preds / total_count\n",
    "                    )\n",
    "                )\n",
    "            correct_preds, total_count = 0, 0\n",
    "            start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4beee5-dde7-437a-ba25-35088204df07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
