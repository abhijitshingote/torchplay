{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# For tips on running notebooks in Google Colab, see\n",
    "# https://pytorch.org/tutorials/beginner/colab\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "import time\n",
    "import torchtext.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext.datasets import AG_NEWS\n",
    "\n",
    "train_iter = iter(AG_NEWS(split=\"train\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "train_iter = AG_NEWS(split=\"train\")\n",
    "\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for _, text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "text_pipeline = lambda x: vocab(tokenizer(x))\n",
    "label_pipeline = lambda x: int(x) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# from torch.utils.data import DataLoader\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# def collate_batch(batch):\n",
    "#     label_list, text_list, offsets = [], [], [0]\n",
    "#     for _label, _text in batch:\n",
    "#         label_list.append(label_pipeline(_label))\n",
    "#         processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "#         text_list.append(processed_text)\n",
    "#         offsets.append(processed_text.size(0))\n",
    "#     label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "#     offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "#     text_list = torch.cat(text_list)\n",
    "#     return label_list.to(device), text_list.to(device), offsets.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# from torch import nn\n",
    "# from torchtext.data.functional import to_map_style_dataset\n",
    "# import time\n",
    "\n",
    "# class TextClassificationModel(nn.Module):\n",
    "#     def __init__(self, vocab_size, embed_dim, num_class):\n",
    "#         super(TextClassificationModel, self).__init__()\n",
    "#         self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=False)\n",
    "#         self.fc = nn.Linear(embed_dim, num_class)\n",
    "#         self.init_weights()\n",
    "\n",
    "#     def init_weights(self):\n",
    "#         initrange = 0.5\n",
    "#         self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "#         self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "#         self.fc.bias.data.zero_()\n",
    "\n",
    "#     def forward(self, text, offsets):\n",
    "#         embedded = self.embedding(text, offsets)\n",
    "#         return self.fc(embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# train_iter = AG_NEWS(split=\"train\")\n",
    "# train_dataset = to_map_style_dataset(train_iter)\n",
    "# num_class = len(set([label for (label, text) in train_iter]))\n",
    "# dataloader = DataLoader(\n",
    "#     train_dataset, batch_size=64, shuffle=False, collate_fn=collate_batch\n",
    "# )\n",
    "# vocab_size = len(vocab)\n",
    "# emsize = 64\n",
    "# model = TextClassificationModel(vocab_size, emsize, num_class).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Hyperparameters\n",
    "# EPOCHS = 10  # epoch\n",
    "# LR = 5  # learning rate\n",
    "# BATCH_SIZE = 64  # batch size for training\n",
    "# epoch=1\n",
    "\n",
    "# criterion = torch.nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.train()\n",
    "# total_acc, total_count = 0, 0\n",
    "# log_interval = 500\n",
    "# start_time = time.time()\n",
    "\n",
    "# for idx, (label, text, offsets) in enumerate(dataloader):\n",
    "#     optimizer.zero_grad()\n",
    "#     predicted_label = model(text, offsets)\n",
    "#     loss = criterion(predicted_label, label)\n",
    "#     loss.backward()\n",
    "#     torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "#     optimizer.step()\n",
    "#     total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "#     total_count += label.size(0)\n",
    "#     if idx % log_interval == 0 and idx > 0:\n",
    "#         elapsed = time.time() - start_time\n",
    "#         print(\n",
    "#             \"| epoch {:3d} | {:5d}/{:5d} batches \"\n",
    "#             \"| accuracy {:8.3f}\".format(\n",
    "#                 epoch, idx, len(dataloader), total_acc / total_count\n",
    "#             )\n",
    "#         )\n",
    "#         total_acc, total_count = 0, 0\n",
    "#         start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abi's collate\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list= [], []\n",
    "    for _label, _text in batch:\n",
    "        label_list.append(label_pipeline(_label))\n",
    "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "        text_list.append(processed_text)\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    text_list=[t.tolist() for t in text_list]\n",
    "    text_list = T.ToTensor(0)(text_list)\n",
    "    return label_list, text_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 1]),\n",
       " tensor([[ 282, 1913,  262],\n",
       "         [ 282, 1913,  136]]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collate_batch(\n",
    "    [\n",
    "        [1,\"i am sports\"],\n",
    "        [2,\"i am news\"]\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingBagClassifier(nn.Module):\n",
    "    def __init__(self,vocab_size,embed_dim,num_classes):\n",
    "        super(EmbeddingBagClassifier,self).__init__()\n",
    "        self.embedding=nn.EmbeddingBag(vocab_size,embed_dim,padding_idx=0)\n",
    "        self.fc=nn.Linear(embed_dim,num_classes)\n",
    "        self.init_weights()\n",
    "\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "    def forward(self,x):\n",
    "        x=self.embedding(x)\n",
    "        out=self.fc(x)\n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = AG_NEWS(split=\"train\")\n",
    "train_dataset = to_map_style_dataset(train_iter)\n",
    "num_class = len(set([label for (label, text) in train_iter]))\n",
    "dataloader = DataLoader(\n",
    "    train_dataset, batch_size=64, shuffle=False, collate_fn=collate_batch\n",
    ")\n",
    "vocab_size = len(vocab)\n",
    "emsize = 64\n",
    "model = EmbeddingBagClassifier(vocab_size, emsize, num_class).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Hyperparameters\n",
    "EPOCHS = 10  # epoch\n",
    "LR = 5  # learning rate\n",
    "BATCH_SIZE = 64  # batch size for training\n",
    "\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   500/ 1875 batches | accuracy    0.669\n",
      "| epoch   1 |  1000/ 1875 batches | accuracy    0.843\n",
      "| epoch   1 |  1500/ 1875 batches | accuracy    0.879\n",
      "| epoch   2 |   500/ 1875 batches | accuracy    0.880\n",
      "| epoch   2 |  1000/ 1875 batches | accuracy    0.893\n",
      "| epoch   2 |  1500/ 1875 batches | accuracy    0.914\n",
      "| epoch   3 |   500/ 1875 batches | accuracy    0.904\n",
      "| epoch   3 |  1000/ 1875 batches | accuracy    0.908\n",
      "| epoch   3 |  1500/ 1875 batches | accuracy    0.926\n",
      "| epoch   4 |   500/ 1875 batches | accuracy    0.915\n",
      "| epoch   4 |  1000/ 1875 batches | accuracy    0.916\n",
      "| epoch   4 |  1500/ 1875 batches | accuracy    0.934\n",
      "| epoch   5 |   500/ 1875 batches | accuracy    0.923\n",
      "| epoch   5 |  1000/ 1875 batches | accuracy    0.923\n",
      "| epoch   5 |  1500/ 1875 batches | accuracy    0.941\n",
      "| epoch   6 |   500/ 1875 batches | accuracy    0.929\n",
      "| epoch   6 |  1000/ 1875 batches | accuracy    0.928\n",
      "| epoch   6 |  1500/ 1875 batches | accuracy    0.945\n",
      "| epoch   7 |   500/ 1875 batches | accuracy    0.934\n",
      "| epoch   7 |  1000/ 1875 batches | accuracy    0.933\n",
      "| epoch   7 |  1500/ 1875 batches | accuracy    0.949\n",
      "| epoch   8 |   500/ 1875 batches | accuracy    0.938\n",
      "| epoch   8 |  1000/ 1875 batches | accuracy    0.937\n",
      "| epoch   8 |  1500/ 1875 batches | accuracy    0.953\n",
      "| epoch   9 |   500/ 1875 batches | accuracy    0.942\n",
      "| epoch   9 |  1000/ 1875 batches | accuracy    0.940\n",
      "| epoch   9 |  1500/ 1875 batches | accuracy    0.956\n",
      "| epoch  10 |   500/ 1875 batches | accuracy    0.945\n",
      "| epoch  10 |  1000/ 1875 batches | accuracy    0.943\n",
      "| epoch  10 |  1500/ 1875 batches | accuracy    0.959\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "total_acc, total_count = 0, 0\n",
    "log_interval = 500\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(1,EPOCHS+1):\n",
    "    for idx, (label, text) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        predicted_label = model(text)\n",
    "        loss = criterion(predicted_label, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "        optimizer.step()\n",
    "        total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "        total_count += label.size(0)\n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print(\n",
    "                \"| epoch {:3d} | {:5d}/{:5d} batches \"\n",
    "                \"| accuracy {:8.3f}\".format(\n",
    "                    epoch, idx, len(dataloader), total_acc / total_count\n",
    "                )\n",
    "            )\n",
    "            total_acc, total_count = 0, 0\n",
    "            start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Important Observations \n",
    "## 1. Train Accuracy jumped from unmoving 25% to about 78% after 10 epochs when applied the learning rate changes\n",
    "```\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
    "```\n",
    "insert clip gradient norm between backward and step\n",
    "```\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "        optimizer.step()\n",
    "```\n",
    "## 2. Train Accuracy jumped from 78% to 96% on passing in the padding_idx to Embedding Bag.\n",
    "Obviously when the embedding bag sums/means the weights of the embeddings\n",
    "we want it to ignore the weights of the padding index\n",
    "padding is just a convenience feature for pytorch training\n",
    "```\n",
    "self.embedding=nn.EmbeddingBag(vocab_size,embed_dim,padding_idx=0)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RNN has no initialized weights\n",
    "but does have learning rate sceduling & gradient clipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingBagClassifier(nn.Module):\n",
    "    def __init__(self,vocab_size,embed_dim,num_classes):\n",
    "        super(EmbeddingBagClassifier,self).__init__()\n",
    "        self.embedding=nn.EmbeddingBag(vocab_size,embed_dim,padding_idx=0)\n",
    "        self.fc=nn.Linear(embed_dim,num_classes)\n",
    "        self.init_weights()\n",
    "\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "    def forward(self,x):\n",
    "        x=self.embedding(x)\n",
    "        out=self.fc(x)\n",
    "        return out\n",
    "\n",
    "vocab_size=len(vocab)\n",
    "embed_dim=300\n",
    "num_classes=4\n",
    "embedbagmodel=EmbeddingBagClassifier(vocab_size,embed_dim,num_classes)\n",
    "\n",
    "# Hyperparameters\n",
    "EPOCHS = 10  # epoch\n",
    "LR = 5  # learning rate\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(embedbagmodel.parameters(), lr=LR, momentum=0.1)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
    "\n",
    "#### TRAIN#####\n",
    "embedbagmodel.train()\n",
    "epochs=10\n",
    "correct_preds, total_count = 0, 0\n",
    "log_interval = 500\n",
    "start_time = time.time()\n",
    "for epoch in range(1,epochs+1):\n",
    "    for idx,(tensors,targets) in enumerate(train_iter):\n",
    "        tensors,targets=tensors,targets\n",
    "        optimizer.zero_grad()\n",
    "        predicted_labels=embedbagmodel(tensors)\n",
    "        loss=criterion(predicted_labels,targets)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(embedbagmodel.parameters(), 0.1)\n",
    "        optimizer.step()\n",
    "        correct_preds +=(predicted_labels.argmax(1)==targets).sum().item()\n",
    "        total_count += targets.size(0)\n",
    "        if idx%log_interval==0 and idx>0:\n",
    "            elapsed=time.time() - start_time\n",
    "            print(\n",
    "            \"| epoch {:3d} | {:5d}/{:5d} batches \"\n",
    "                    \"| accuracy {:8.3f}\".format(\n",
    "                        ##### Weird - in order for len(train_iter) to work data_pipe must be converted using to_map_style_dataset\n",
    "                        #### after which you cannot do data_pipe.map(func) even more weird\n",
    "                        #### to circumvent the transform will need to go into the collate_fn\n",
    "                        #### so all transform inside collate_fn and then len(dataloader) will also work as long as data_pipe was converted\n",
    "                        #### to dataset using to_map_style_dataset\n",
    "                        epoch, idx, len(train_iter), correct_preds / total_count\n",
    "                    )\n",
    "                )\n",
    "            correct_preds, total_count = 0, 0\n",
    "            start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
