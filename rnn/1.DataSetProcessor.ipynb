{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "188c87f1-097e-46cc-8ecc-05179228df57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda install -y -c conda-forge spacy\n",
    "# !python -m spacy download en_core_web_sm\n",
    "# !python -m spacy download de_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf076098-a11b-4f93-a810-e73fe175b0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchdata.datapipes as dp\n",
    "import torchtext.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef2b1f83-fa8d-4c43-b898-780efa0aed4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from torchtext.vocab import build_vocab_from_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7daf2edc-b745-4ff3-8458-a433dfd79be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng = spacy.load(\"en_core_web_sm\") # Load the English model to tokenize English text\n",
    "de = spacy.load(\"de_core_news_sm\") # Load the German model to tokenize German text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3ac3a71a-9f2d-4837-a1eb-1c47b7e35361",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi', 'how', 'are', 'you']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[token.text for token in eng.tokenizer(\"Hi how are you\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a98d9cf0-cea6-4320-86d9-9b16b8c902b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH = 'deu-eng/deu.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1fcb8683-892b-41ea-b14e-e7f863c5e7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pipe= dp.iter.IterableWrapper([FILE_PATH])\n",
    "data_pipe= dp.iter.FileOpener(data_pipe,mode='rb')\n",
    "data_pipe=data_pipe.parse_csv(skip_lines=0, delimiter='\\t', as_tuple=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0171ae25-23db-4d98-aed3-acefe5ebb90d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Go.', 'Geh.')\n",
      "('Hi.', 'Hallo!')\n",
      "('Hi.', 'Grüß Gott!')\n",
      "('Run!', 'Lauf!')\n",
      "('Run.', 'Lauf!')\n",
      "('Wow!', 'Potzdonner!')\n",
      "('Wow!', 'Donnerwetter!')\n",
      "('Duck!', 'Kopf runter!')\n",
      "('Fire!', 'Feuer!')\n",
      "('Help!', 'Hilfe!')\n",
      "('Help!', 'Zu Hülf!')\n",
      "('Stay.', 'Bleib!')\n",
      "('Stop!', 'Stopp!')\n",
      "('Stop!', 'Anhalten!')\n",
      "('Wait!', 'Warte!')\n",
      "('Wait.', 'Warte.')\n",
      "('Begin.', 'Fang an.')\n",
      "('Do it.', 'Mache es!')\n",
      "('Do it.', 'Tue es.')\n",
      "('Go on.', 'Mach weiter.')\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "for sample in data_pipe:\n",
    "    print(sample)\n",
    "    i+=1\n",
    "    if i==20:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "209fc9c8-6a69-44ef-991e-81cdf4bda849",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeAttribution(row):\n",
    "    return row[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d17135c2-5475-4761-9579-93c7880dcf72",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pipe=data_pipe.map(removeAttribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "16767018-3e32-4988-a03e-1291668cda53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Go.', 'Geh.')\n"
     ]
    }
   ],
   "source": [
    "for sample in data_pipe:\n",
    "    print(sample)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f2076e33-5c17-4541-8ebb-fed699375ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def engTokenize(text):\n",
    "    return [token.text for token in eng.tokenizer(text)]\n",
    "\n",
    "def deTokenize(text):\n",
    "    return [token.text for token in de.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "edb64756-0d17-4df6-b165-3cc162efe76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTokens(data_iter, place):\n",
    "    \"\"\"\n",
    "    Function to yield tokens from an iterator. Since, our iterator contains\n",
    "    tuple of sentences (source and target), `place` parameters defines for which\n",
    "    index to return the tokens for. `place=0` for source and `place=1` for target\n",
    "    \"\"\"\n",
    "    for english, german in data_iter:\n",
    "        if place == 0:\n",
    "            yield engTokenize(english)\n",
    "        else:\n",
    "            yield deTokenize(german)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7c2d9819-ea3a-4696-b31e-aa894c357d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_vocab = build_vocab_from_iterator(\n",
    "    getTokens(data_pipe,0),\n",
    "    min_freq=2,\n",
    "    specials= ['<pad>', '<sos>', '<eos>', '<unk>'],\n",
    "    special_first=True\n",
    ")\n",
    "source_vocab.set_default_index(source_vocab['<unk>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a91b5824-dbf0-4b9d-a2d3-54909fb31e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_vocab = build_vocab_from_iterator(\n",
    "    getTokens(data_pipe,1),\n",
    "    min_freq=2,\n",
    "    specials= ['<pad>', '<sos>', '<eos>', '<unk>'],\n",
    "    special_first=True\n",
    ")\n",
    "target_vocab.set_default_index(target_vocab['<unk>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5b3bde97-c690-46c2-b715-df7a7e362dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTransform(vocab):\n",
    "    \"\"\"\n",
    "    Create transforms based on given vocabulary. The returned transform is applied to sequence\n",
    "    of tokens.\n",
    "    \"\"\"\n",
    "    text_tranform = T.Sequential(\n",
    "        ## converts the sentences to indices based on given vocabulary\n",
    "        T.VocabTransform(vocab=vocab),\n",
    "        ## Add <sos> at beginning of each sentence. 1 because the index for <sos> in vocabulary is\n",
    "        # 1 as seen in previous section\n",
    "        T.AddToken(1, begin=True),\n",
    "        ## Add <eos> at beginning of each sentence. 2 because the index for <eos> in vocabulary is\n",
    "        # 2 as seen in previous section\n",
    "        T.AddToken(2, begin=False)\n",
    "    )\n",
    "    return text_tranform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4885dbba-253a-460c-9064-d894f90277e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([1, 616, 4, 2], [1, 739, 4, 2])\n"
     ]
    }
   ],
   "source": [
    "def applyTransform(sequence_pair):\n",
    "    \"\"\"\n",
    "    Apply transforms to sequence of tokens in a sequence pair\n",
    "    \"\"\"\n",
    "\n",
    "    return (\n",
    "        getTransform(source_vocab)(engTokenize(sequence_pair[0])),\n",
    "        getTransform(target_vocab)(deTokenize(sequence_pair[1]))\n",
    "    )\n",
    "data_pipe = data_pipe.map(applyTransform) ## Apply the function to each element in the iterator\n",
    "temp_list = list(data_pipe)\n",
    "print(temp_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fd57facf-7393-4304-ab7d-6610350af839",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sortBucket(bucket):\n",
    "    \"\"\"\n",
    "    Function to sort a given bucket. Here, we want to sort based on the length of\n",
    "    source and target sequence.\n",
    "    \"\"\"\n",
    "    return sorted(bucket, key=lambda x: (len(x[0]), len(x[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b521d7e0-322f-4d0e-aee7-3bfa33cb0c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pipe = data_pipe.bucketbatch(\n",
    "    batch_size = 4, batch_num=5,  bucket_num=1,\n",
    "    use_in_batch_shuffle=False, sort_key=sortBucket\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e4f8e8e6-5735-411f-b54f-b1925023d7f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[([1, 11105, 17, 4, 2], [1, 507, 29, 24, 2]), ([1, 11105, 17, 4, 2], [1, 7994, 1487, 24, 2]), ([1, 5335, 21, 4, 2], [1, 6956, 32, 24, 2]), ([1, 5335, 21, 4, 2], [1, 16003, 32, 24, 2])]\n"
     ]
    }
   ],
   "source": [
    "print(list(data_pipe)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "888a6c01-0a15-4c7e-a88f-577d185143cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert from  [(X_1,y_1), (X_2,y_2), (X_3,y_3), (X_4,y_4)] to ((X_1,X_2,X_3,X_4), (y_1,y_2,y_3,y_4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5ac286c8-381a-4113-864c-1c54c56f1026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(([1, 1066, 4, 2], [1, 2989, 4, 2], [1, 3, 194, 2], [1, 1670, 194, 2]), ([1, 1383, 4, 2], [1, 6030, 1616, 24, 2], [1, 740, 2445, 24, 2], [1, 1210, 3, 24, 2]))\n"
     ]
    }
   ],
   "source": [
    "def separateSourceTarget(sequence_pairs):\n",
    "    \"\"\"\n",
    "    input of form: `[(X_1,y_1), (X_2,y_2), (X_3,y_3), (X_4,y_4)]`\n",
    "    output of form: `((X_1,X_2,X_3,X_4), (y_1,y_2,y_3,y_4))`\n",
    "    \"\"\"\n",
    "    sources,targets = zip(*sequence_pairs)\n",
    "    return sources,targets\n",
    "\n",
    "## Apply the function to each element in the iterator\n",
    "data_pipe = data_pipe.map(separateSourceTarget)\n",
    "print(list(data_pipe)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b631ad50-9a26-434f-a3da-42d7ba81db30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def applyPadding(pair_of_sequences):\n",
    "    \"\"\"\n",
    "    Convert sequences to tensors and apply padding\n",
    "    \"\"\"\n",
    "    return (T.ToTensor(0)(list(pair_of_sequences[0])), T.ToTensor(0)(list(pair_of_sequences[1])))\n",
    "## `T.ToTensor(0)` returns a transform that converts the sequence to `torch.tensor` and also applies\n",
    "# padding. Here, `0` is passed to the constructor to specify the index of the `<pad>` token in the\n",
    "# vocabulary.\n",
    "data_pipe = data_pipe.map(applyPadding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f2773850-cf61-4e58-b781-ed464ed6b2fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source:  <sos> Relax . <eos> <pad>\n",
      "Traget:  <sos> Entspann dich . <eos>\n",
      "Source:  <sos> I see . <eos>\n",
      "Traget:  <sos> Aha . <eos> <pad>\n",
      "Source:  <sos> I ran . <eos>\n",
      "Traget:  <sos> Ich rannte . <eos>\n",
      "Source:  <sos> I see . <eos>\n",
      "Traget:  <sos> Ich verstehe . <eos>\n"
     ]
    }
   ],
   "source": [
    "source_index_to_string = source_vocab.get_itos()\n",
    "target_index_to_string = target_vocab.get_itos()\n",
    "\n",
    "def showSomeTransformedSentences(data_pipe):\n",
    "    \"\"\"\n",
    "    Function to show how the sentences look like after applying all transforms.\n",
    "    Here we try to print actual words instead of corresponding index\n",
    "    \"\"\"\n",
    "    for sources,targets in data_pipe:\n",
    "        if sources[0][-1] != 0:\n",
    "            continue # Just to visualize padding of shorter sentences\n",
    "        for i in range(4):\n",
    "            source = \"\"\n",
    "            for token in sources[i]:\n",
    "                source += \" \" + source_index_to_string[token]\n",
    "            target = \"\"\n",
    "            for token in targets[i]:\n",
    "                target += \" \" + target_index_to_string[token]\n",
    "            print(f\"Source: {source}\")\n",
    "            print(f\"Traget: {target}\")\n",
    "        break\n",
    "\n",
    "showSomeTransformedSentences(data_pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c47915de-b73e-41e0-bb16-dcaa72acb8e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[   1, 6860,   23,  194,    2],\n",
      "        [   1, 6860,   23,   10,    2],\n",
      "        [   1, 1042,   21,    4,    2],\n",
      "        [   1, 1042,   21,    4,    2]]), tensor([[    1, 17926,    24,     2,     0],\n",
      "        [    1,     3,     8,     2,     0],\n",
      "        [    1,  1578,    32,    24,     2],\n",
      "        [    1,  3817,    32,    24,     2]]))\n"
     ]
    }
   ],
   "source": [
    "for sample in data_pipe:\n",
    "    print(sample)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c4cc3f-c10d-4a11-ba07-e56ef9eb49d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
